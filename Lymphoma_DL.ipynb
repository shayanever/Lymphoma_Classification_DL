{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5XUVMxVFUHso",
      "metadata": {
        "id": "5XUVMxVFUHso"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hNSUK3_oUEjm",
      "metadata": {
        "id": "hNSUK3_oUEjm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/lymhphoma')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd3a5d95-5298-40ff-a3d0-3eb36baf86d8",
      "metadata": {
        "id": "dd3a5d95-5298-40ff-a3d0-3eb36baf86d8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score\n",
        "import psutil\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import GPUtil\n",
        "from PIL import Image\n",
        "import h5py\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from skimage.color import rgb2lab, rgb2hed\n",
        "from pathlib import Path\n",
        "from skimage.exposure import rescale_intensity\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from skimage.util import view_as_windows\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from keras.optimizers import RMSprop\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from tensorflow.keras.layers import Add, Multiply, Lambda, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.optimizers import legacy as legacy_optimizers\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PmRIomxAE0mg",
      "metadata": {
        "id": "PmRIomxAE0mg"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow keras scikit-image matplotlib seaborn scikit-learn gputil scikit-learn scikit-metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "555249cd-403f-49a1-b666-f3dc20624690",
      "metadata": {
        "id": "555249cd-403f-49a1-b666-f3dc20624690"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce8fe14d-ce10-435b-8538-88a50a25a6f3",
      "metadata": {
        "id": "ce8fe14d-ce10-435b-8538-88a50a25a6f3"
      },
      "outputs": [],
      "source": [
        "mixed_precision.set_global_policy('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "883bfc8b-d4d3-4f8a-9e74-4816dd7dffc6",
      "metadata": {
        "id": "883bfc8b-d4d3-4f8a-9e74-4816dd7dffc6"
      },
      "outputs": [],
      "source": [
        "devices = tf.config.list_physical_devices()\n",
        "print(\"\\nDevices: \", devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d60fa8-d430-45cf-8132-e2bd9ca9e135",
      "metadata": {
        "id": "67d60fa8-d430-45cf-8132-e2bd9ca9e135"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        details = tf.config.experimental.get_device_details(gpus[0])\n",
        "        print(\"GPU details: \", details)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    print(\"No GPU found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bac06cf-db5f-4863-9090-ef1ea0238463",
      "metadata": {
        "id": "6bac06cf-db5f-4863-9090-ef1ea0238463"
      },
      "outputs": [],
      "source": [
        "def load_data(data_dir, color, class_list):\n",
        "    conversion_functions = {\n",
        "        'rgb': lambda img_path: Image.open(img_path).convert('RGB'),\n",
        "        'grayscale': lambda img_path: Image.open(img_path).convert('L'),\n",
        "        'lab': lambda img_path: rgb2lab(np.array(Image.open(img_path).convert('RGB')))\n",
        "    }\n",
        "\n",
        "    X, Y = [], []\n",
        "    for class_name in class_list:\n",
        "        for filename in data_dir.glob(f'{class_name}/*.tif'):\n",
        "            img_path = str(filename)\n",
        "            if color in conversion_functions:\n",
        "                converted_img = conversion_functions[color](img_path)\n",
        "                X.append(np.array(converted_img))\n",
        "            Y.append(class_name)\n",
        "\n",
        "    X = np.asarray(X, dtype=np.float32)\n",
        "    Y = np.asarray(Y)\n",
        "\n",
        "    if len(X.shape) == 3:\n",
        "        X = X.reshape((X.shape[0], X.shape[1], X.shape[2], 1))\n",
        "\n",
        "    target = np.zeros(Y.shape, dtype=int)\n",
        "    target[Y == 'CLL'] = 0\n",
        "    target[Y == 'FL'] = 1\n",
        "    target[Y == 'MCL'] = 2\n",
        "    Y = target\n",
        "\n",
        "    if color in ['rgb', 'lab', 'grayscale']:\n",
        "        X = X / 255.0\n",
        "\n",
        "    print(f\"Loaded data for color space {color} with shape {X.shape}\")\n",
        "    return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8qrQpk9pBDXt",
      "metadata": {
        "id": "8qrQpk9pBDXt"
      },
      "outputs": [],
      "source": [
        "def rebuild_model(model_path, custom_objects):\n",
        "    with open(model_path.replace('.h5', '_config.json'), 'r') as json_file:\n",
        "        json_config = json_file.read()\n",
        "    model = tf.keras.models.model_from_json(json_config, custom_objects=custom_objects)\n",
        "    model.load_weights(model_path)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aa16d64-f6af-4b8b-a793-90cb8b4a368e",
      "metadata": {
        "id": "9aa16d64-f6af-4b8b-a793-90cb8b4a368e"
      },
      "outputs": [],
      "source": [
        "def save_patches(patches, labels, prefix, batch_size=10000):\n",
        "    num_batches = len(patches) // batch_size + 1\n",
        "    for i in range(num_batches):\n",
        "        batch_patches = patches[i*batch_size:(i+1)*batch_size]\n",
        "        batch_labels = labels[i*batch_size:(i+1)*batch_size]\n",
        "        patch_file = f'{prefix}_patches_{i}.npy'\n",
        "        label_file = f'{prefix}_labels_{i}.npy'\n",
        "        np.save(patch_file, batch_patches)\n",
        "        np.save(label_file, batch_labels)\n",
        "        print(f\"Saved patches to {patch_file} and labels to {label_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a041fccd-1464-412b-b496-56792c7217e2",
      "metadata": {
        "id": "a041fccd-1464-412b-b496-56792c7217e2"
      },
      "outputs": [],
      "source": [
        "def create_patches(X, Y, size, stride, prefix, batch_size=10000):\n",
        "    os.makedirs(prefix, exist_ok=True)\n",
        "    patches, labels = [], []\n",
        "\n",
        "    for i in range(X.shape[0]):\n",
        "        label = Y[i]\n",
        "        img_patches = view_as_windows(X[i], (size, size, X.shape[3]), step=stride)\n",
        "        nR, nC, _, H, W, C = img_patches.shape\n",
        "        img_patches = img_patches.reshape(nR * nC, H, W, C)\n",
        "        patches.extend(img_patches)\n",
        "        labels.extend([label] * img_patches.shape[0])\n",
        "\n",
        "    patches = np.asarray(patches, dtype=np.float32)\n",
        "    labels = np.asarray(labels)\n",
        "    save_patches(patches, labels, prefix, batch_size=batch_size)\n",
        "    print(f\"Created and saved patches for prefix {prefix} with shape {patches.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fab53b74-ef26-42ae-b28d-4da25122f5d4",
      "metadata": {
        "id": "fab53b74-ef26-42ae-b28d-4da25122f5d4"
      },
      "outputs": [],
      "source": [
        "class Attention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='attention_weight',\n",
        "                                 shape=(input_shape[-1], input_shape[-1]),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        self.b = self.add_weight(name='attention_bias',\n",
        "                                 shape=(input_shape[-1],),\n",
        "                                 initializer='zeros',\n",
        "                                 trainable=True)\n",
        "        self.u = self.add_weight(name='context_vector',\n",
        "                                 shape=(input_shape[-1], 1),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        super(Attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        u_it = tf.tanh(tf.tensordot(x, self.W, axes=[-1, 0]) + self.b)\n",
        "        scores = tf.tensordot(u_it, self.u, axes=[-1, 0])\n",
        "        alphas = tf.nn.softmax(scores, axis=1)\n",
        "        context = tf.reduce_sum(alphas * x, axis=1)\n",
        "        return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03eb866c-de23-4a74-a555-19f15909bcb9",
      "metadata": {
        "id": "03eb866c-de23-4a74-a555-19f15909bcb9"
      },
      "outputs": [],
      "source": [
        "def resnet_block(inputs, filters, kernel_size=3, stride=1, conv_shortcut=True, name=None):\n",
        "    bn_axis = 3 if tf.keras.backend.image_data_format() == 'channels_last' else 1\n",
        "    x = Conv2D(filters, kernel_size, strides=stride, padding='same', use_bias=False, name=name + '_conv1')(inputs)\n",
        "    x = BatchNormalization(axis=bn_axis, name=name + '_bn1')(x)\n",
        "    x = Activation('relu', name=name + '_relu1')(x)\n",
        "    x = Conv2D(filters, kernel_size, padding='same', use_bias=False, name=name + '_conv2')(x)\n",
        "    x = BatchNormalization(axis=bn_axis, name=name + '_bn2')(x)\n",
        "    if conv_shortcut:\n",
        "        shortcut = Conv2D(filters, 1, strides=stride, use_bias=False, name=name + '_0_conv')(inputs)\n",
        "        shortcut = BatchNormalization(axis=bn_axis, name=name + '_0_bn')(shortcut)\n",
        "        x = Add(name=name + '_add')([x, shortcut])\n",
        "    else:\n",
        "        x = Add(name=name + '_add')([x, inputs])\n",
        "    x = Activation('relu', name=name + '_out')(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "646ca12c-efb3-455f-a321-e5db1f425d74",
      "metadata": {
        "id": "646ca12c-efb3-455f-a321-e5db1f425d74"
      },
      "outputs": [],
      "source": [
        "def create_resnet_model(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(64, 7, strides=2, padding='same', use_bias=False, name='conv1_conv')(inputs)\n",
        "    x = BatchNormalization(axis=3, name='conv1_bn')(x)\n",
        "    x = Activation('relu', name='conv1_relu')(x)\n",
        "    x = MaxPooling2D(3, strides=2, padding='same', name='pool1_pool')(x)\n",
        "    x = resnet_block(x, 64, name='conv2_block1')\n",
        "    x = resnet_block(x, 64, name='conv2_block2')\n",
        "    x = resnet_block(x, 64, name='conv2_block3')\n",
        "    x = resnet_block(x, 128, stride=2, name='conv3_block1')\n",
        "    x = resnet_block(x, 128, name='conv3_block2')\n",
        "    x = resnet_block(x, 128, name='conv3_block3')\n",
        "    x = resnet_block(x, 128, name='conv3_block4')\n",
        "    x = resnet_block(x, 256, stride=2, name='conv4_block1')\n",
        "    x = resnet_block(x, 256, name='conv4_block2')\n",
        "    x = resnet_block(x, 256, name='conv4_block3')\n",
        "    x = resnet_block(x, 256, name='conv4_block4')\n",
        "    x = resnet_block(x, 256, name='conv4_block5')\n",
        "    x = resnet_block(x, 256, name='conv4_block6')\n",
        "    x = resnet_block(x, 512, stride=2, name='conv5_block1')\n",
        "    x = resnet_block(x, 512, name='conv5_block2')\n",
        "    x = resnet_block(x, 512, name='conv5_block3')\n",
        "    x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "    x = tf.expand_dims(x, axis=1) \n",
        "    x = Attention()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax', name='fc1000')(x)\n",
        "    model = Model(inputs, outputs, name='resnet50')\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8e6b33e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_features(model, x_data):\n",
        "    feature_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
        "    features = feature_model.predict(x_data)\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84ff6fff",
      "metadata": {},
      "outputs": [],
      "source": [
        "def grad_cam(model, img, layer_name=\"conv2d_3\"):\n",
        "    grad_model = Model([model.inputs], [model.get_layer(layer_name).output, model.output])\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(np.array([img]))\n",
        "        loss = predictions[:, np.argmax(predictions[0])]\n",
        "    output = conv_outputs[0]\n",
        "    grads = tape.gradient(loss, conv_outputs)[0]\n",
        "    weights = tf.reduce_mean(grads, axis=(0, 1))\n",
        "    cam = np.dot(output, weights)\n",
        "    cam = cv2.resize(cam.numpy(), (img.shape[1], img.shape[0]))\n",
        "    cam = np.maximum(cam, 0)\n",
        "    heatmap = (cam - cam.min()) / (cam.max() - cam.min())\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    superimposed_img = cv2.addWeighted(cv2.cvtColor(img, cv2.COLOR_RGB2BGR), 0.6, heatmap, 0.4, 0)\n",
        "    return superimposed_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2ede0b3-b222-41d6-9ffa-e6a6cf42d25e",
      "metadata": {
        "id": "b2ede0b3-b222-41d6-9ffa-e6a6cf42d25e"
      },
      "outputs": [],
      "source": [
        "def create_cnn_model_with_attention(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(filters=32, kernel_size=(5, 5), strides=(1, 1), padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
        "    x = Conv2D(filters=48, kernel_size=(5, 5), strides=(1, 1), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\n",
        "    x = Conv2D(filters=64, kernel_size=(5, 5), strides=(1, 1), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\n",
        "    x = Conv2D(filters=128, kernel_size=(5, 5), strides=(1, 1), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Reshape((64, 1))(x)\n",
        "    x = Attention()(x)\n",
        "    x = Flatten()(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2d3d5cf-005d-4e20-bfee-9c87483f38f3",
      "metadata": {
        "id": "a2d3d5cf-005d-4e20-bfee-9c87483f38f3"
      },
      "outputs": [],
      "source": [
        "def create_rnn_model_with_attention(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    if input_shape[-1] == 1:\n",
        "        x = Reshape(input_shape[0:2])(inputs)\n",
        "    else:\n",
        "        x = Reshape((input_shape[0], input_shape[1] * input_shape[2]))(inputs)\n",
        "        x = TimeDistributed(Flatten())(x)\n",
        "    x = GRU(256, activation='relu', return_sequences=True)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = GRU(128, activation='relu', return_sequences=True)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Attention()(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50affb93-7a82-449e-b028-3a815ae8ef86",
      "metadata": {
        "id": "50affb93-7a82-449e-b028-3a815ae8ef86"
      },
      "outputs": [],
      "source": [
        "def create_cnn_rnn_model_with_attention(input_shape, num_classes):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(filters=32, kernel_size=(5, 5), strides=(1, 1), padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
        "    x = Conv2D(filters=48, kernel_size=(5, 5), strides=(1, 1), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\n",
        "    x = Conv2D(filters=64, kernel_size=(5, 5), strides=(1, 1), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\n",
        "    x = Conv2D(filters=128, kernel_size=(5, 5), strides=(1, 1), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Reshape((x.shape[1] * x.shape[2], x.shape[3]))(x)\n",
        "    x = GRU(256, activation='relu', return_sequences=True)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = GRU(128, activation='relu', return_sequences=True)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Attention()(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eb5f907-d087-4998-b758-9df73f606440",
      "metadata": {
        "id": "2eb5f907-d087-4998-b758-9df73f606440"
      },
      "outputs": [],
      "source": [
        "class PrintEpochMetrics(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        print(f\"Epoch {epoch + 1} Accuracy: {logs.get('accuracy'):.4f}, \"\n",
        "              f\"Val Accuracy: {logs.get('val_accuracy'):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6618f401-f269-4a37-80e6-759acdc7163c",
      "metadata": {
        "id": "6618f401-f269-4a37-80e6-759acdc7163c"
      },
      "outputs": [],
      "source": [
        "def reset_energy_usage():\n",
        "    return {'epoch': [], 'time': [], 'cpu_percent': [], 'memory_percent': [], 'gpu_percent': [], 'gpu_memory_used': []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3aff4b-ea05-4eeb-ac93-4243ff811f3e",
      "metadata": {
        "id": "0d3aff4b-ea05-4eeb-ac93-4243ff811f3e"
      },
      "outputs": [],
      "source": [
        "def log_energy_usage(epoch, energy_usage, start_time):\n",
        "    elapsed_time = time.time() - start_time\n",
        "    cpu_percent = psutil.cpu_percent(interval=1)\n",
        "    memory_percent = psutil.virtual_memory().percent\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu_stats = gpus[0]\n",
        "        gpu_percent = gpu_stats.load * 100\n",
        "        gpu_memory_used = gpu_stats.memoryUsed\n",
        "    else:\n",
        "        gpu_percent = 0\n",
        "        gpu_memory_used = 0\n",
        "\n",
        "    energy_usage['epoch'].append(epoch)\n",
        "    energy_usage['time'].append(elapsed_time)\n",
        "    energy_usage['cpu_percent'].append(cpu_percent)\n",
        "    energy_usage['memory_percent'].append(memory_percent)\n",
        "    energy_usage['gpu_percent'].append(gpu_percent)\n",
        "    energy_usage['gpu_memory_used'].append(gpu_memory_used)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f02ae1e7-7a22-4a3b-98fa-9cde86b6a3c4",
      "metadata": {
        "id": "f02ae1e7-7a22-4a3b-98fa-9cde86b6a3c4"
      },
      "outputs": [],
      "source": [
        "class EnergyUsageCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, energy_usage, start_time):\n",
        "        self.energy_usage = energy_usage\n",
        "        self.start_time = start_time\n",
        "        super().__init__()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        log_energy_usage(epoch, self.energy_usage, self.start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cd2d8e9-9375-48dc-9fd4-868c282b482b",
      "metadata": {
        "id": "7cd2d8e9-9375-48dc-9fd4-868c282b482b"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(rotation_range=20,\n",
        "                             width_shift_range=0.2,\n",
        "                             height_shift_range=0.2,\n",
        "                             shear_range=0.2,\n",
        "                             zoom_range=0.2,\n",
        "                             horizontal_flip=True,\n",
        "                             fill_mode='nearest')\n",
        "\n",
        "val_datagen = ImageDataGenerator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fdd3391-c3c4-4419-8e5f-6144ab723596",
      "metadata": {
        "id": "6fdd3391-c3c4-4419-8e5f-6144ab723596"
      },
      "outputs": [],
      "source": [
        "def infinite_generator(generator):\n",
        "    while True:\n",
        "        for batch in generator:\n",
        "            yield batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6668914c-2d1f-4600-9bff-b511303fef12",
      "metadata": {
        "id": "6668914c-2d1f-4600-9bff-b511303fef12"
      },
      "outputs": [],
      "source": [
        "def train_and_save_model(model_fn, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, model_name):\n",
        "    energy_usage = reset_energy_usage()\n",
        "\n",
        "    model = model_fn(input_shape, num_classes)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='accuracy', patience=3, verbose=1, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=3, min_lr=0.0001, verbose=1)\n",
        "    print_metrics = PrintEpochMetrics()\n",
        "\n",
        "    global start_time\n",
        "    start_time = time.time()\n",
        "\n",
        "    energy_usage_callback = EnergyUsageCallback(energy_usage, start_time)\n",
        "\n",
        "    history = model.fit(train_generator,\n",
        "                        steps_per_epoch=steps_per_epoch,\n",
        "                        validation_data=val_generator,\n",
        "                        validation_steps=validation_steps,\n",
        "                        epochs=20,\n",
        "                        verbose=1,\n",
        "                        callbacks=[early_stopping, reduce_lr, print_metrics, energy_usage_callback])\n",
        "\n",
        "    model_save_path = f'models/{model_name}.h5'\n",
        "    model.save(model_save_path)\n",
        "    print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "    energy_usage_df = pd.DataFrame(energy_usage)\n",
        "    energy_usage_df.to_csv(f'energy_usage_metrics_{model_name}.csv', index=False)\n",
        "    print(f\"Energy usage metrics saved to energy_usage_metrics_{model_name}.csv\")\n",
        "\n",
        "    del model\n",
        "    K.clear_session()\n",
        "\n",
        "    return model_save_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bab04000-08bd-4e62-bbb4-c1564382936f",
      "metadata": {
        "id": "bab04000-08bd-4e62-bbb4-c1564382936f"
      },
      "outputs": [],
      "source": [
        "def reshape(X_patches, Y_patches, mode='eval'):\n",
        "    X_reshaped = X_patches.reshape(-1, X_patches.shape[1], X_patches.shape[2], X_patches.shape[3])\n",
        "    if mode == 'test':\n",
        "        Y_reshaped = Y_patches.reshape(-1, 1)\n",
        "    else:\n",
        "        Y_reshaped = Y_patches\n",
        "    return X_reshaped, Y_reshaped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf80d0c6-2250-4545-9638-b0dbe0177141",
      "metadata": {
        "id": "cf80d0c6-2250-4545-9638-b0dbe0177141"
      },
      "outputs": [],
      "source": [
        "def majority_voting(model, X_test, Y_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    predicted_classes = np.argmax(predictions, axis=1)\n",
        "    cm_prediction = predicted_classes.reshape(Y_test.shape[0], -1)\n",
        "    cm_winner = [np.argmax(np.bincount(patch_preds)) for patch_preds in cm_prediction]\n",
        "    return cm_prediction, cm_winner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6799374-8ff3-474e-ad67-37c7b2232736",
      "metadata": {
        "id": "a6799374-8ff3-474e-ad67-37c7b2232736"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in np.ndindex(cm.shape):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15caaa37-4a10-4385-b695-918e6922e2a6",
      "metadata": {
        "id": "15caaa37-4a10-4385-b695-918e6922e2a6"
      },
      "outputs": [],
      "source": [
        "data_dir = Path('dataset/')\n",
        "color_spaces = ['rgb', 'grayscale', 'lab']\n",
        "class_list = ['CLL', 'FL', 'MCL']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47bf0dfe-1ef7-439d-bfa9-b288d69d7ad4",
      "metadata": {
        "id": "47bf0dfe-1ef7-439d-bfa9-b288d69d7ad4"
      },
      "outputs": [],
      "source": [
        "all_model_paths = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "203bd888-9fc2-40b8-90de-e51d75b491b1",
      "metadata": {
        "id": "203bd888-9fc2-40b8-90de-e51d75b491b1"
      },
      "outputs": [],
      "source": [
        "patch_size_train_val = 36\n",
        "stride_train_val = 32\n",
        "patch_size_test = 36\n",
        "stride_test = 28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c3fb2f-1588-4a77-a826-4da32887a5cd",
      "metadata": {
        "id": "19c3fb2f-1588-4a77-a826-4da32887a5cd"
      },
      "outputs": [],
      "source": [
        "color = 'rgb'\n",
        "X, Y = load_data(data_dir, color, class_list)\n",
        "\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.4, random_state=seed, stratify=Y)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=seed, stratify=Y_temp)\n",
        "\n",
        "create_patches(X_train, Y_train, size=patch_size_train_val, stride=stride_train_val, prefix=f'patches_{color}/train_patches_{color}')\n",
        "create_patches(X_val, Y_val, size=patch_size_train_val, stride=stride_train_val, prefix=f'patches_{color}/val_patches_{color}')\n",
        "create_patches(X_test, Y_test, size=patch_size_test, stride=stride_test, prefix=f'patches_{color}/test_patches_{color}')\n",
        "\n",
        "train_patches_files = sorted(Path('.').glob(f'patches_{color}/train_patches_{color}_patches_*.npy'))\n",
        "val_patches_files = sorted(Path('.').glob(f'patches_{color}/val_patches_{color}_patches_*.npy'))\n",
        "test_patches_files = sorted(Path('.').glob(f'patches_{color}/test_patches_{color}_patches_*.npy'))\n",
        "\n",
        "train_labels_files = sorted(Path('.').glob(f'patches_{color}/train_patches_{color}_labels_*.npy'))\n",
        "val_labels_files = sorted(Path('.').glob(f'patches_{color}/val_patches_{color}_labels_*.npy'))\n",
        "test_labels_files = sorted(Path('.').glob(f'patches_{color}/test_patches_{color}_labels_*.npy'))\n",
        "\n",
        "if train_patches_files and val_patches_files and test_patches_files:\n",
        "    X_train_patches = np.vstack([np.load(f) for f in train_patches_files])\n",
        "    Y_train_patches = np.hstack([np.load(f) for f in train_labels_files])\n",
        "\n",
        "    X_val_patches = np.vstack([np.load(f) for f in val_patches_files])\n",
        "    Y_val_patches = np.hstack([np.load(f) for f in val_labels_files])\n",
        "\n",
        "    X_test_patches = np.vstack([np.load(f) for f in test_patches_files])\n",
        "    Y_test_patches = np.hstack([np.load(f) for f in test_labels_files])\n",
        "\n",
        "    print(f\"Loaded patches for color space {color} with shapes: \")\n",
        "    print(f\"  X_train_patches: {X_train_patches.shape}\")\n",
        "    print(f\"  Y_train_patches: {Y_train_patches.shape}\")\n",
        "    print(f\"  X_val_patches: {X_val_patches.shape}\")\n",
        "    print(f\"  Y_val_patches: {Y_val_patches.shape}\")\n",
        "    print(f\"  X_test_patches: {X_test_patches.shape}\")\n",
        "    print(f\"  Y_test_patches: {Y_test_patches.shape}\")\n",
        "\n",
        "    train_generator = datagen.flow(X_train_patches, Y_train_patches, batch_size=32, shuffle=True)\n",
        "    train_generator = infinite_generator(train_generator)\n",
        "\n",
        "    val_generator = val_datagen.flow(X_val_patches, Y_val_patches, batch_size=32, shuffle=True)\n",
        "    val_generator = infinite_generator(val_generator)\n",
        "\n",
        "    steps_per_epoch = len(X_train_patches) // 32\n",
        "    validation_steps = len(X_val_patches) // 32\n",
        "else:\n",
        "    print(f\"Skipping color space {color} due to missing patches.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ef8f20-795c-484c-a5ad-ddd27667681a",
      "metadata": {
        "id": "92ef8f20-795c-484c-a5ad-ddd27667681a"
      },
      "outputs": [],
      "source": [
        "train_generator = datagen.flow(X_train_patches, Y_train_patches, batch_size=32, shuffle=True)\n",
        "train_generator = infinite_generator(train_generator)\n",
        "val_generator = val_datagen.flow(X_val_patches, Y_val_patches, batch_size=32, shuffle=True)\n",
        "val_generator = infinite_generator(val_generator)\n",
        "steps_per_epoch = len(X_train_patches) // 32\n",
        "validation_steps = len(X_val_patches) // 32\n",
        "input_shape = X_train_patches.shape[1:]\n",
        "num_classes = len(class_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ee56ed2-ca16-4041-a264-430937debf33",
      "metadata": {
        "id": "8ee56ed2-ca16-4041-a264-430937debf33"
      },
      "outputs": [],
      "source": [
        "cnn_model_path = train_and_save_model(create_cnn_model_with_attention, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'cnn_attention_{color}')\n",
        "all_model_paths.append(cnn_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bd66d08-e77d-4220-bf63-7f34d21e838b",
      "metadata": {
        "id": "4bd66d08-e77d-4220-bf63-7f34d21e838b"
      },
      "outputs": [],
      "source": [
        "rnn_model_path = train_and_save_model(create_rnn_model_with_attention, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'rnn_attention_{color}')\n",
        "all_model_paths.append(rnn_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8386359e-7b69-425f-928d-75226e5962a6",
      "metadata": {
        "id": "8386359e-7b69-425f-928d-75226e5962a6"
      },
      "outputs": [],
      "source": [
        "cnn_rnn_model_path = train_and_save_model(create_cnn_rnn_model_with_attention, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'cnn_rnn_attention_{color}')\n",
        "all_model_paths.append(cnn_rnn_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba967c21-ef3c-485d-ac0c-29e72759c297",
      "metadata": {
        "id": "ba967c21-ef3c-485d-ac0c-29e72759c297"
      },
      "outputs": [],
      "source": [
        "resnet_model_path = train_and_save_model(create_resnet_model, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'resnet_{color}')\n",
        "all_model_paths.append(resnet_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323ac085-1a12-418f-934c-4e5a95252323",
      "metadata": {
        "id": "323ac085-1a12-418f-934c-4e5a95252323"
      },
      "outputs": [],
      "source": [
        "color = 'rgb'\n",
        "X, Y = load_data(data_dir, color, class_list)\n",
        "\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.4, random_state=seed, stratify=Y)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=seed, stratify=Y_temp)\n",
        "\n",
        "create_patches(X_train, Y_train, size=patch_size_train_val, stride=stride_train_val, prefix=f'patches_{color}/train_patches_{color}')\n",
        "create_patches(X_val, Y_val, size=patch_size_train_val, stride=stride_train_val, prefix=f'patches_{color}/val_patches_{color}')\n",
        "create_patches(X_test, Y_test, size=patch_size_test, stride=stride_test, prefix=f'patches_{color}/test_patches_{color}')\n",
        "\n",
        "train_patches_files = sorted(Path('.').glob(f'patches_{color}/train_patches_{color}_patches_*.npy'))\n",
        "val_patches_files = sorted(Path('.').glob(f'patches_{color}/val_patches_{color}_patches_*.npy'))\n",
        "test_patches_files = sorted(Path('.').glob(f'patches_{color}/test_patches_{color}_patches_*.npy'))\n",
        "\n",
        "train_labels_files = sorted(Path('.').glob(f'patches_{color}/train_patches_{color}_labels_*.npy'))\n",
        "val_labels_files = sorted(Path('.').glob(f'patches_{color}/val_patches_{color}_labels_*.npy'))\n",
        "test_labels_files = sorted(Path('.').glob(f'patches_{color}/test_patches_{color}_labels_*.npy'))\n",
        "\n",
        "print(f\"Train patches files: {train_patches_files}\")\n",
        "print(f\"Val patches files: {val_patches_files}\")\n",
        "print(f\"Test patches files: {test_patches_files}\")\n",
        "\n",
        "print(f\"Train labels files: {train_labels_files}\")\n",
        "print(f\"Val labels files: {val_labels_files}\")\n",
        "print(f\"Test labels files: {test_labels_files}\")\n",
        "\n",
        "if train_patches_files and val_patches_files and test_patches_files:\n",
        "    X_train_patches = np.vstack([np.load(f) for f in train_patches_files])\n",
        "    Y_train_patches = np.hstack([np.load(f) for f in train_labels_files])\n",
        "\n",
        "    X_val_patches = np.vstack([np.load(f) for f in val_patches_files])\n",
        "    Y_val_patches = np.hstack([np.load(f) for f in val_labels_files])\n",
        "\n",
        "    X_test_patches = np.vstack([np.load(f) for f in test_patches_files])\n",
        "    Y_test_patches = np.hstack([np.load(f) for f in test_labels_files])\n",
        "\n",
        "    print(f\"Loaded patches for color space {color} with shapes: \")\n",
        "    print(f\"  X_train_patches: {X_train_patches.shape}\")\n",
        "    print(f\"  Y_train_patches: {Y_train_patches.shape}\")\n",
        "    print(f\"  X_val_patches: {X_val_patches.shape}\")\n",
        "    print(f\"  Y_val_patches: {Y_val_patches.shape}\")\n",
        "    print(f\"  X_test_patches: {X_test_patches.shape}\")\n",
        "    print(f\"  Y_test_patches: {Y_test_patches.shape}\")\n",
        "\n",
        "    train_generator = datagen.flow(X_train_patches, Y_train_patches, batch_size=32, shuffle=True)\n",
        "    train_generator = infinite_generator(train_generator)\n",
        "\n",
        "    val_generator = val_datagen.flow(X_val_patches, Y_val_patches, batch_size=32, shuffle=True)\n",
        "    val_generator = infinite_generator(val_generator)\n",
        "\n",
        "    steps_per_epoch = len(X_train_patches) // 32\n",
        "    validation_steps = len(X_val_patches) // 32\n",
        "else:\n",
        "    print(f\"Skipping color space {color} due to missing patches.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e19116e-5a45-4caf-8cca-48494e2ca235",
      "metadata": {
        "id": "7e19116e-5a45-4caf-8cca-48494e2ca235"
      },
      "outputs": [],
      "source": [
        "train_generator = datagen.flow(X_train_patches, Y_train_patches, batch_size=32, shuffle=True)\n",
        "train_generator = infinite_generator(train_generator)\n",
        "val_generator = val_datagen.flow(X_val_patches, Y_val_patches, batch_size=32, shuffle=True)\n",
        "val_generator = infinite_generator(val_generator)\n",
        "steps_per_epoch = len(X_train_patches) // 32\n",
        "validation_steps = len(X_val_patches) // 32\n",
        "input_shape = X_train_patches.shape[1:]\n",
        "num_classes = len(class_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47dc73af-ee07-40b5-a039-b92af5ad361a",
      "metadata": {
        "id": "47dc73af-ee07-40b5-a039-b92af5ad361a"
      },
      "outputs": [],
      "source": [
        "cnn_model_path = train_and_save_model(create_cnn_model_with_attention, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'cnn_attention_{color}')\n",
        "all_model_paths.append(cnn_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea62f113-ea34-48bf-999b-b3a76ee112e4",
      "metadata": {
        "id": "ea62f113-ea34-48bf-999b-b3a76ee112e4"
      },
      "outputs": [],
      "source": [
        "rnn_model_path = train_and_save_model(create_rnn_model_with_attention, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'rnn_attention_{color}')\n",
        "all_model_paths.append(rnn_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c385da8-e750-4648-ba5d-8791110b8493",
      "metadata": {
        "id": "5c385da8-e750-4648-ba5d-8791110b8493"
      },
      "outputs": [],
      "source": [
        "cnn_rnn_model_path = train_and_save_model(create_cnn_rnn_model_with_attention, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'cnn_rnn_attention_{color}')\n",
        "all_model_paths.append(cnn_rnn_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0752085c-f8de-468d-bc95-1738a5119208",
      "metadata": {
        "id": "0752085c-f8de-468d-bc95-1738a5119208"
      },
      "outputs": [],
      "source": [
        "resnet_model_path = train_and_save_model(create_resnet_model, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'resnet_{color}')\n",
        "all_model_paths.append(resnet_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f09e9e7e",
      "metadata": {
        "id": "f09e9e7e"
      },
      "outputs": [],
      "source": [
        "color = 'lab'\n",
        "X, Y = load_data(data_dir, color, class_list)\n",
        "\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.4, random_state=seed, stratify=Y)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=seed, stratify=Y_temp)\n",
        "\n",
        "create_patches(X_train, Y_train, size=patch_size_train_val, stride=stride_train_val, prefix=f'patches_{color}/train_patches_{color}')\n",
        "create_patches(X_val, Y_val, size=patch_size_train_val, stride=stride_train_val, prefix=f'patches_{color}/val_patches_{color}')\n",
        "create_patches(X_test, Y_test, size=patch_size_test, stride=stride_test, prefix=f'patches_{color}/test_patches_{color}')\n",
        "\n",
        "train_patches_files = sorted(Path('.').glob(f'patches_{color}/train_patches_{color}_patches_*.npy'))\n",
        "val_patches_files = sorted(Path('.').glob(f'patches_{color}/val_patches_{color}_patches_*.npy'))\n",
        "test_patches_files = sorted(Path('.').glob(f'patches_{color}/test_patches_{color}_patches_*.npy'))\n",
        "\n",
        "train_labels_files = sorted(Path('.').glob(f'patches_{color}/train_patches_{color}_labels_*.npy'))\n",
        "val_labels_files = sorted(Path('.').glob(f'patches_{color}/val_patches_{color}_labels_*.npy'))\n",
        "test_labels_files = sorted(Path('.').glob(f'patches_{color}/test_patches_{color}_labels_*.npy'))\n",
        "\n",
        "print(f\"Train patches files: {train_patches_files}\")\n",
        "print(f\"Val patches files: {val_patches_files}\")\n",
        "print(f\"Test patches files: {test_patches_files}\")\n",
        "\n",
        "print(f\"Train labels files: {train_labels_files}\")\n",
        "print(f\"Val labels files: {val_labels_files}\")\n",
        "print(f\"Test labels files: {test_labels_files}\")\n",
        "\n",
        "if train_patches_files and val_patches_files and test_patches_files:\n",
        "    X_train_patches = np.vstack([np.load(f) for f in train_patches_files])\n",
        "    Y_train_patches = np.hstack([np.load(f) for f in train_labels_files])\n",
        "\n",
        "    X_val_patches = np.vstack([np.load(f) for f in val_patches_files])\n",
        "    Y_val_patches = np.hstack([np.load(f) for f in val_labels_files])\n",
        "\n",
        "    X_test_patches = np.vstack([np.load(f) for f in test_patches_files])\n",
        "    Y_test_patches = np.hstack([np.load(f) for f in test_labels_files])\n",
        "\n",
        "    print(f\"Loaded patches for color space {color} with shapes: \")\n",
        "    print(f\"  X_train_patches: {X_train_patches.shape}\")\n",
        "    print(f\"  Y_train_patches: {Y_train_patches.shape}\")\n",
        "    print(f\"  X_val_patches: {X_val_patches.shape}\")\n",
        "    print(f\"  Y_val_patches: {Y_val_patches.shape}\")\n",
        "    print(f\"  X_test_patches: {X_test_patches.shape}\")\n",
        "    print(f\"  Y_test_patches: {Y_test_patches.shape}\")\n",
        "\n",
        "    train_generator = datagen.flow(X_train_patches, Y_train_patches, batch_size=32, shuffle=True)\n",
        "    train_generator = infinite_generator(train_generator)\n",
        "\n",
        "    val_generator = val_datagen.flow(X_val_patches, Y_val_patches, batch_size=32, shuffle=True)\n",
        "    val_generator = infinite_generator(val_generator)\n",
        "\n",
        "    steps_per_epoch = len(X_train_patches) // 32\n",
        "    validation_steps = len(X_val_patches) // 32\n",
        "else:\n",
        "    print(f\"Skipping color space {color} due to missing patches.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n-BJ8mPC39ix",
      "metadata": {
        "id": "n-BJ8mPC39ix"
      },
      "outputs": [],
      "source": [
        "train_generator = datagen.flow(X_train_patches, Y_train_patches, batch_size=32, shuffle=True)\n",
        "train_generator = infinite_generator(train_generator)\n",
        "val_generator = val_datagen.flow(X_val_patches, Y_val_patches, batch_size=32, shuffle=True)\n",
        "val_generator = infinite_generator(val_generator)\n",
        "steps_per_epoch = len(X_train_patches) // 32\n",
        "validation_steps = len(X_val_patches) // 32\n",
        "input_shape = X_train_patches.shape[1:]\n",
        "num_classes = len(class_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b273f71",
      "metadata": {
        "id": "1b273f71"
      },
      "outputs": [],
      "source": [
        "cnn_model_path = train_and_save_model(create_cnn_model_with_attention, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'cnn_attention_{color}')\n",
        "all_model_paths.append(cnn_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29429d8a",
      "metadata": {
        "id": "29429d8a"
      },
      "outputs": [],
      "source": [
        "rnn_model_path = train_and_save_model(create_rnn_model_with_attention, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'rnn_attention_{color}')\n",
        "all_model_paths.append(rnn_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa37f04d",
      "metadata": {
        "id": "fa37f04d"
      },
      "outputs": [],
      "source": [
        "cnn_rnn_model_path = train_and_save_model(create_cnn_rnn_model_with_attention, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'cnn_rnn_attention_{color}')\n",
        "all_model_paths.append(cnn_rnn_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f76d2911",
      "metadata": {
        "id": "f76d2911"
      },
      "outputs": [],
      "source": [
        "resnet_model_path = train_and_save_model(create_resnet_model, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'resnet_{color}')\n",
        "all_model_paths.append(resnet_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ncI0sXzz3nRA",
      "metadata": {
        "id": "ncI0sXzz3nRA"
      },
      "outputs": [],
      "source": [
        "color = 'grayscale'\n",
        "X, Y = load_data(data_dir, color, class_list)\n",
        "\n",
        "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.4, random_state=seed, stratify=Y)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=seed, stratify=Y_temp)\n",
        "\n",
        "create_patches(X_train, Y_train, size=patch_size_train_val, stride=stride_train_val, prefix=f'patches_{color}/train_patches_{color}')\n",
        "create_patches(X_val, Y_val, size=patch_size_train_val, stride=stride_train_val, prefix=f'patches_{color}/val_patches_{color}')\n",
        "create_patches(X_test, Y_test, size=patch_size_test, stride=stride_test, prefix=f'patches_{color}/test_patches_{color}')\n",
        "\n",
        "train_patches_files = sorted(Path('.').glob(f'patches_{color}/train_patches_{color}_patches_*.npy'))\n",
        "val_patches_files = sorted(Path('.').glob(f'patches_{color}/val_patches_{color}_patches_*.npy'))\n",
        "test_patches_files = sorted(Path('.').glob(f'patches_{color}/test_patches_{color}_patches_*.npy'))\n",
        "\n",
        "train_labels_files = sorted(Path('.').glob(f'patches_{color}/train_patches_{color}_labels_*.npy'))\n",
        "val_labels_files = sorted(Path('.').glob(f'patches_{color}/val_patches_{color}_labels_*.npy'))\n",
        "test_labels_files = sorted(Path('.').glob(f'patches_{color}/test_patches_{color}_labels_*.npy'))\n",
        "\n",
        "print(f\"Train patches files: {train_patches_files}\")\n",
        "print(f\"Val patches files: {val_patches_files}\")\n",
        "print(f\"Test patches files: {test_patches_files}\")\n",
        "\n",
        "print(f\"Train labels files: {train_labels_files}\")\n",
        "print(f\"Val labels files: {val_labels_files}\")\n",
        "print(f\"Test labels files: {test_labels_files}\")\n",
        "\n",
        "if train_patches_files and val_patches_files and test_patches_files:\n",
        "    X_train_patches = np.vstack([np.load(f) for f in train_patches_files])\n",
        "    Y_train_patches = np.hstack([np.load(f) for f in train_labels_files])\n",
        "\n",
        "    X_val_patches = np.vstack([np.load(f) for f in val_patches_files])\n",
        "    Y_val_patches = np.hstack([np.load(f) for f in val_labels_files])\n",
        "\n",
        "    X_test_patches = np.vstack([np.load(f) for f in test_patches_files])\n",
        "    Y_test_patches = np.hstack([np.load(f) for f in test_labels_files])\n",
        "\n",
        "    print(f\"Loaded patches for color space {color} with shapes: \")\n",
        "    print(f\"  X_train_patches: {X_train_patches.shape}\")\n",
        "    print(f\"  Y_train_patches: {Y_train_patches.shape}\")\n",
        "    print(f\"  X_val_patches: {X_val_patches.shape}\")\n",
        "    print(f\"  Y_val_patches: {Y_val_patches.shape}\")\n",
        "    print(f\"  X_test_patches: {X_test_patches.shape}\")\n",
        "    print(f\"  Y_test_patches: {Y_test_patches.shape}\")\n",
        "\n",
        "    train_generator = datagen.flow(X_train_patches, Y_train_patches, batch_size=32, shuffle=True)\n",
        "    train_generator = infinite_generator(train_generator)\n",
        "\n",
        "    val_generator = val_datagen.flow(X_val_patches, Y_val_patches, batch_size=32, shuffle=True)\n",
        "    val_generator = infinite_generator(val_generator)\n",
        "\n",
        "    steps_per_epoch = len(X_train_patches) // 32\n",
        "    validation_steps = len(X_val_patches) // 32\n",
        "else:\n",
        "    print(f\"Skipping color space {color} due to missing patches.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ul7mS8sc3-t3",
      "metadata": {
        "id": "ul7mS8sc3-t3"
      },
      "outputs": [],
      "source": [
        "train_generator = datagen.flow(X_train_patches, Y_train_patches, batch_size=32, shuffle=True)\n",
        "train_generator = infinite_generator(train_generator)\n",
        "val_generator = val_datagen.flow(X_val_patches, Y_val_patches, batch_size=32, shuffle=True)\n",
        "val_generator = infinite_generator(val_generator)\n",
        "steps_per_epoch = len(X_train_patches) // 32\n",
        "validation_steps = len(X_val_patches) // 32\n",
        "input_shape = X_train_patches.shape[1:]\n",
        "num_classes = len(class_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pb8lQjbN3uDQ",
      "metadata": {
        "id": "Pb8lQjbN3uDQ"
      },
      "outputs": [],
      "source": [
        "cnn_model_path = train_and_save_model(create_cnn_model_with_attention, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'cnn_attention_{color}')\n",
        "all_model_paths.append(cnn_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BRgwFp-G3udl",
      "metadata": {
        "id": "BRgwFp-G3udl"
      },
      "outputs": [],
      "source": [
        "rnn_model_path = train_and_save_model(create_rnn_model_with_attention, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'rnn_attention_{color}')\n",
        "all_model_paths.append(rnn_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TqUyMaSi3wHx",
      "metadata": {
        "id": "TqUyMaSi3wHx"
      },
      "outputs": [],
      "source": [
        "cnn_rnn_model_path = train_and_save_model(create_cnn_rnn_model_with_attention, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'cnn_rnn_attention_{color}')\n",
        "all_model_paths.append(cnn_rnn_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rZP0iMsN3xp2",
      "metadata": {
        "id": "rZP0iMsN3xp2"
      },
      "outputs": [],
      "source": [
        "resnet_model_path = train_and_save_model(create_resnet_model, input_shape, num_classes, train_generator, val_generator, steps_per_epoch, validation_steps, f'resnet_{color}')\n",
        "all_model_paths.append(resnet_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nai0J1fSYHvk",
      "metadata": {
        "id": "Nai0J1fSYHvk"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model_path, X_test_patches, Y_test_patches, color):\n",
        "    try:\n",
        "        custom_objects = {'Attention': Attention}\n",
        "        model = load_model(model_path, custom_objects=custom_objects)\n",
        "        X_test_eval, Y_test_eval = reshape(X_test_patches, Y_test_patches)\n",
        "        score = model.evaluate(X_test_eval, Y_test_eval, verbose=0)\n",
        "        print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
        "\n",
        "        X_test, Y_test = reshape(X_test_patches, Y_test_patches, mode='test')\n",
        "        cm_prediction, cm_winner = majority_voting(model, X_test, Y_test)\n",
        "        cm_prediction = np.asarray(cm_prediction).reshape(cm_prediction.shape[0] * cm_prediction.shape[1])\n",
        "        cm_patches = confusion_matrix(Y_test_eval, cm_prediction)\n",
        "        cm_img = confusion_matrix(Y_test[:, 0], cm_winner)\n",
        "\n",
        "        print(f'Confusion matrix for patches for color {color}.')\n",
        "        plot_confusion_matrix(cm_patches, classes=class_list, title=f'Confusion Matrix for patches ({color})')\n",
        "        print(f'Confusion matrix for images for color {color}.')\n",
        "        plot_confusion_matrix(cm_img, classes=class_list, title=f'Confusion Matrix for images ({color})', cmap=plt.cm.Reds)\n",
        "\n",
        "        report = classification_report(Y_test[:, 0], cm_winner, target_names=class_list)\n",
        "        print(f'Classification report for color {color}:')\n",
        "        print(report)\n",
        "\n",
        "        y_pred = model.predict(X_test_patches)\n",
        "        return y_pred\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while evaluating model {model_path}: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "029a5989-0a19-422e-a3f6-0aa0d94fce60",
      "metadata": {
        "id": "029a5989-0a19-422e-a3f6-0aa0d94fce60"
      },
      "outputs": [],
      "source": [
        "color_spaces = ['rgb', 'grayscale', 'lab']\n",
        "all_model_paths = {\n",
        "    'rgb': [\n",
        "        'models/cnn_attention_rgb.h5',\n",
        "        'models/rnn_attention_rgb.h5',\n",
        "        'models/cnn_rnn_attention_rgb.h5',\n",
        "        'models/resnet_rgb.h5'\n",
        "    ],\n",
        "    'grayscale': [\n",
        "        'models/cnn_attention_grayscale.h5',\n",
        "        'models/rnn_attention_grayscale.h5',\n",
        "        'models/cnn_rnn_attention_grayscale.h5',\n",
        "        'models/resnet_grayscale.h5'\n",
        "    ],\n",
        "    'lab': [\n",
        "        'models/cnn_attention_lab.h5',\n",
        "        'models/rnn_attention_lab.h5',\n",
        "        'models/cnn_rnn_attention_lab.h5',\n",
        "        'models/resnet_lab.h5'\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZuAUR-W0X4m5",
      "metadata": {
        "id": "ZuAUR-W0X4m5"
      },
      "outputs": [],
      "source": [
        "ensemble_predictions = {color: [] for color in color_spaces}\n",
        "X_test_patches_dict = {}\n",
        "Y_test_patches_dict = {}\n",
        "\n",
        "for color in color_spaces:\n",
        "    test_patches_files = sorted(Path(f'patches_{color}').glob(f'test_patches_{color}_patches_*.npy'))\n",
        "    test_labels_files = sorted(Path(f'patches_{color}').glob(f'test_patches_{color}_labels_*.npy'))\n",
        "\n",
        "    if test_patches_files and test_labels_files:\n",
        "        X_test_patches = np.vstack([np.load(f) for f in test_patches_files])\n",
        "        Y_test_patches = np.hstack([np.load(f) for f in test_labels_files])\n",
        "\n",
        "        X_test_patches_dict[color] = X_test_patches\n",
        "        Y_test_patches_dict[color] = Y_test_patches\n",
        "\n",
        "        print(f\"Loaded patches for color space {color} with shapes: \")\n",
        "        print(f\"  X_test_patches: {X_test_patches.shape}\")\n",
        "        print(f\"  Y_test_patches: {Y_test_patches.shape}\")\n",
        "\n",
        "        for model_path in all_model_paths[color]:\n",
        "            print(f'Evaluating model {model_path}')\n",
        "            try:\n",
        "                y_pred = evaluate_model(model_path, X_test_patches, Y_test_patches, color)\n",
        "                ensemble_predictions[color].append(y_pred)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to evaluate model {model_path}: {e}\")\n",
        "    else:\n",
        "        print(f\"No patches found for color space {color}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ltfbwV3lGh",
      "metadata": {
        "id": "f5ltfbwV3lGh"
      },
      "outputs": [],
      "source": [
        "def get_ensemble_predictions(predictions_list):\n",
        "    return np.mean(predictions_list, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0fd36ff",
      "metadata": {},
      "source": [
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T8swJ17c4Vge",
      "metadata": {
        "id": "T8swJ17c4Vge"
      },
      "outputs": [],
      "source": [
        "final_ensemble_predictions = []\n",
        "for color in color_spaces:\n",
        "    if ensemble_predictions[color]:\n",
        "        ensemble_pred = get_ensemble_predictions(ensemble_predictions[color])\n",
        "        final_ensemble_predictions.append(ensemble_pred)\n",
        "    else:\n",
        "        print(f\"No predictions available for color space {color}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J7d0b5xI4XFs",
      "metadata": {
        "id": "J7d0b5xI4XFs"
      },
      "outputs": [],
      "source": [
        "if final_ensemble_predictions:\n",
        "    merged_ensemble_pred = np.mean(final_ensemble_predictions, axis=0)\n",
        "    merged_ensemble_pred_labels = np.argmax(merged_ensemble_pred, axis=1)\n",
        "\n",
        "    Y_test_combined = np.hstack([Y_test_patches_dict[color] for color in color_spaces if color in Y_test_patches_dict])\n",
        "    accuracy = accuracy_score(Y_test_combined, merged_ensemble_pred_labels)\n",
        "    print(f'Merged ensemble accuracy: {accuracy:.2f}')\n",
        "\n",
        "    np.save('merged_ensemble_predictions.npy', merged_ensemble_pred)\n",
        "else:\n",
        "    print(\"No ensemble predictions available to merge.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "869874b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "features_train_rgb = np.concatenate([extract_features(model, X_test_patches_dict['rgb']) for model in models_rgb], axis=1)\n",
        "features_train_gray = np.concatenate([extract_features(model, X_test_patches_dict['gray']) for model in models_gray], axis=1)\n",
        "features_train_lab = np.concatenate([extract_features(model, X_test_patches_dict['lab']) for model in models_lab], axis=1)\n",
        "features_train = np.concatenate([features_train_rgb, features_train_gray, features_train_lab], axis=1)\n",
        "\n",
        "features_test_rgb = np.concatenate([extract_features(model, X_test_patches_dict['rgb']) for model in models_rgb], axis=1)\n",
        "features_test_gray = np.concatenate([extract_features(model, X_test_patches_dict['gray']) for model in models_gray], axis=1)\n",
        "features_test_lab = np.concatenate([extract_features(model, X_test_patches_dict['lab']) for model in models_lab], axis=1)\n",
        "features_test = np.concatenate([features_test_rgb, features_test_gray, features_test_lab], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4838e710",
      "metadata": {},
      "outputs": [],
      "source": [
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('lr', log_reg),\n",
        "    ('knn', knn)\n",
        "], voting='soft')\n",
        "\n",
        "voting_clf.fit(features_train, np.argmax(Y_test_patches_dict['rgb'], axis=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56de65b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "voting_clf.fit(features_train, np.argmax(Y_test_patches_dict['rgb'], axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff169774",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Ensemble Model Classification Report (RGB):\")\n",
        "print(classification_report(np.argmax(Y_test_patches_dict['rgb'], axis=1), ensemble_pred, target_names=['CLL', 'FL', 'MCL']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0860a268",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Ensemble Model Classification Report (Grayscale):\")\n",
        "print(classification_report(np.argmax(Y_test_patches_dict['grayscale'], axis=1), ensemble_pred, target_names=['CLL', 'FL', 'MCL']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a080346c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Ensemble Model Classification Report (LAB):\")\n",
        "print(classification_report(np.argmax(Y_test_patches_dict['lab'], axis=1), ensemble_pred, target_names=['CLL', 'FL', 'MCL']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd0d031f",
      "metadata": {},
      "outputs": [],
      "source": [
        "for color in color_spaces:\n",
        "    if ensemble_predictions[color]:\n",
        "        ensemble_pred = get_ensemble_predictions(ensemble_predictions[color])\n",
        "        ensemble_pred = np.argmax(ensemble_pred, axis=1)\n",
        "        accuracy = accuracy_score(Y_test_patches_dict[color], ensemble_pred)\n",
        "        print(f'Ensemble accuracy for {color} color space: {accuracy:.2f}')\n",
        "    else:\n",
        "        print(f\"No predictions available for color space {color}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d36be9be",
      "metadata": {},
      "outputs": [],
      "source": [
        "for color in color_spaces:\n",
        "    if ensemble_predictions[color]:\n",
        "        ensemble_pred = get_ensemble_predictions(ensemble_predictions[color])\n",
        "        np.save(f'ensemble_predictions_{color}.npy', ensemble_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5a1d221",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_ensemble_predictions = []\n",
        "for color in color_spaces:\n",
        "    if ensemble_predictions[color]:\n",
        "        ensemble_pred = get_ensemble_predictions(ensemble_predictions[color])\n",
        "        final_ensemble_predictions.append(ensemble_pred)\n",
        "    else:\n",
        "        print(f\"No predictions available for color space {color}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8635f1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "if final_ensemble_predictions:\n",
        "    merged_ensemble_pred = np.mean(final_ensemble_predictions, axis=0)\n",
        "    merged_ensemble_pred_labels = np.argmax(merged_ensemble_pred, axis=1)\n",
        "\n",
        "    Y_test_combined = np.hstack([Y_test_patches_dict[color] for color in color_spaces if color in Y_test_patches_dict])\n",
        "    accuracy = accuracy_score(Y_test_combined, merged_ensemble_pred_labels)\n",
        "    print(f'Merged ensemble accuracy: {accuracy:.2f}')\n",
        "\n",
        "    np.save('merged_ensemble_predictions.npy', merged_ensemble_pred)\n",
        "else:\n",
        "    print(\"No ensemble predictions available to merge.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
